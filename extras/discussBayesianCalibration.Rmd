---
title: "Bayesian Calibration Discussion"
author: "Fan Bu"
date: "04/12/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r source functions, echo=FALSE, message=FALSE, warning=FALSE}
source('simpleCalibration.R')
```


## Setup

Hypotheses:
$$
H_0: \beta \leq h \quad \text{  v.s. } \quad H_1: \beta > h.
$$
Usually $h=0$, but can change the hypothesis threshold $h$ value for "calibration" as $\beta$ may be a biased estimand.

What actually happens in our Bayesian sequential test, given a decision threshold $\delta_1$:

1. For analysis time point $t=1:T$, compute 
$$
p_{1t,o} = P(\beta_t > h \mid \text{data at time } t),
$$
for each specific negative control outcome $o$. 

2. Across all time points $t$, $H_1$ will be claimed true for outcome $o$ if $\max(p_{1t,o}) > \delta_1$.

3. Type I error rate $e_1$ is estimated as the proportion among all $o$'s for which $\max(p_{1t,o}) > \delta_1$.


## How to calibrate

To calibrate, we want to limit Type I error rate such that $e_1 \leq \alpha$ --- or, practically, we would prefer that $e_1 \approx \alpha$. 

Two thresholds to potentially calibrate on:

1. Hypothesis threshold $h$ --- i.e., change the hypotheses we practically test to correct for systematic error in order to test the hypotheses we actually want

2. Decision threshold $\delta1$ --- i.e., change the Bayesian decision rule to attain a desired overall (Type I) error rate


However, we also have estimated systematic error distributions from negative control analyses, and we can de-bias (or "adjust") the posterior samples accordingly. Thus, there are in fact **four** variations we can potentially do:

1. Calibrate w.r.t. hypothesis threshold $h$, **with** de-biasing/adjustment

2. Calibrate w.r.t. hypothesis threshold $h$, **without** de-biasing/adjustment

3. Calibrate w.r.t. decision threshold $\delta1$, **with** de-biasing/adjustment

4. Calibrate w.r.t. decision threshold $\delta1$, **without** de-biasing/adjustment


### Brief description: calibrating $h$

1. Choose a reasonable value for $\delta1$ (e.g., $0.95$) and $\alpha$ (e.g., $0.05$)

2. Access all adjusted/unadjusted posterior samples for all negative control outcomes (for a specific analysis design and exposure etc.)

3. Use binary search (within a pre-specified search range) to find a proper $h$ value such that $e_1$ (estimated Type I error rate for these outcomes) gets very close to $\alpha$ (details in appendix)

4. Retain the $h$ value and do further evaluation (e.g., Type 2 error rate) on it

### Brief description: calibrating $\delta_1$

1. Set $h=0$ (probably much more reasonable if using de-biased/adjusted estimates) and choose $\alpha$ (e.g., $0.05$)

2. Find $mp_{o} = \max(p_{1t,o})$'s for all the negative control outcomes of interest (for a specific analysis design and exposure etc.)

3. Set $\delta_1$ as the $1-\alpha$ quantile of all $mp_{o}$ values

4. Retain the $\delta_1$ value and do further evaluation (e.g., Type 2 error rate) on it

### Metrics

1. Type I error (error rates on negative controls)

2. Type II error (error rates on positive controls)

<!-- 3. F1 score -->
<!-- $$ -->
<!-- F_1 = \frac{2}{1/P + 1/R} = \frac{2 P \times R}{P + R}, -->
<!-- $$ -->
<!-- where $P = \frac{TP}{TP+FP}$ precision, $R = 1 - \text{type 2 error}$ recall.  -->


## Examples


```{r paths and other setup, echo=FALSE}
summarypath = '~/Documents/Research/betterResults/summary'
samplepath = "/Volumes/WD-Drive/betterResults-likelihoodProfiles/"
cachepath = '../localCache/'

db = 'CCAE'
eid = 211981
aid = 2
pid = 1
tolerance = 0.004

```

<!-- #### Example 1: without de-biasing/adjustment, `Historical Comparator` design -->

#### Example 1: `Historical Comparator` design, with or without de-biasing/adjustment

```{r ex1, results='hide', cache=TRUE}
res1 = plotCalibration(database_id = db,
                method = 'HistoricalComparator', # 'SCCS'
                analysis_id = aid,
                exposure_id = eid,
                prior_id = pid,
                summaryPath = summarypath,
                samplePath = samplepath,
                cachePath = cachepath,
                tol = tolerance,
                useAdjusted = list(delta1 = FALSE, null=FALSE), 
                showPlots = FALSE)
```

<!-- #### Example 2: with de-biasing/adjustment, `Historical Comparator` design -->

```{r ex2, results='hide',cache=TRUE}
res2 = plotCalibration(database_id = db,
                method = 'HistoricalComparator', # 'SCCS'
                analysis_id = aid,
                exposure_id = eid,
                prior_id = pid,
                summaryPath = summarypath,
                samplePath = samplepath,
                cachePath = cachepath,
                tol = tolerance,
                useAdjusted = list(delta1 = TRUE, null=TRUE),
                showPlots = FALSE)
```

```{r ex1 and ex2 errors, fig.width=13, fig.height=5}
plotSideBySide(res2, res1, plotToShow = 'error')
```

<!-- # ```{r ex1 and ex2 f1 scores, fig.width=7, fig.height=4} -->
<!-- # plotSideBySide(res2, res1, plotToShow = 'f1') -->
<!-- # ``` -->



<!-- #### Example 3: with de-biasing/adjustment, `SCCS` design -->
#### Example 2: `SCCS` design, with or without de-biasing/adjustment

```{r ex3, results='hide',cache=TRUE}
res3 = plotCalibration(database_id = db,
                method = 'SCCS',
                analysis_id = aid,
                exposure_id = eid,
                prior_id = pid,
                summaryPath = summarypath,
                samplePath = samplepath,
                cachePath = cachepath,
                tol = tolerance,
                useAdjusted = list(delta1 = TRUE, null=TRUE),
                showPlots = FALSE)
```

<!-- #### Example 4: without de-biasing/adjustment, `SCCS` design -->
```{r ex4, results='hide',cache=TRUE}
res4 = plotCalibration(database_id = db,
                method = 'SCCS',
                analysis_id = aid,
                exposure_id = eid,
                prior_id = pid,
                summaryPath = summarypath,
                samplePath = samplepath,
                cachePath = cachepath,
                tol = tolerance,
                useAdjusted = list(delta1 = FALSE, null=FALSE),
                showPlots = FALSE)
```

```{r ex3 and ex4 errors, fig.width=13, fig.height=5}
plotSideBySide(res3, res4, plotToShow = 'error')
```

<!-- # ```{r ex3 and ex4 f1 scores, fig.width=7, fig.height=4} -->
<!-- # plotSideBySide(res3, res4, plotToShow = 'f1') -->
<!-- # ``` -->

## Discussion

1. How to justify changing $h$? Is changing the hypothesis mid-through the anlaysis a valid thing to do? 

2. How to interpret Bayesian estimates (with/without adjustments) after the "calibration"?

3. This whole procedure is retrospective calibration --- can't really do this in practice, and it actually suffers the same pre-schedule problem of MaxSPRT.


### Appendix

#### A1. Calibrating $h$: more details

We can show that, given a decision threshold $\delta_1$, the (empirical) Type I error rate ($O$ is the total number of negative controls)
$$
e_{1}(h) = \frac{1}{O} \sum_{o=1}^O I(\max_{t}(p_{1t,o}) > \delta_1)
$$
is a decreasing step function of $h$, with $e_1(h) \rightarrow 1$ as $h \rightarrow -\infty$ and $e_1(h) \rightarrow 0$ as $h \rightarrow \infty$. A rough illustration is shown below.
```{r demo plot,fig.width=5, fig.height=3.5, fig.align='center'}
type1func <- function(x, knots){
  mean(knots > x)
}

kts = runif(50, min = -1, max = 3)
x = seq(from = -1.3, to = 3.3, by = 0.005)
y = sapply(x, type1func, knots = kts)

dat = data.frame(x=x, y=y)

ggplot(dat, aes(x=x,y=y)) +
  geom_line() +
  geom_hline(yintercept = 0.05, color="#547BD3", size = 0.8)+
  #scale_y_continuous(breaks = c(0,0.05, 1))+
  labs(y='Type I error', x='h') +
  theme_bw(base_size = 14)

```

Therefore, for a desired Type I error rate level $\alpha$ (e.g., $\alpha = 0.05$) given a search range $[a,b]$ we can use binary search to find a value $h^*$ such that $\lvert e_1(h^*) - \alpha \rvert < \epsilon$ for a specified tolerance $\epsilon$. 


