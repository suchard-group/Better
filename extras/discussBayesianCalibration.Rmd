---
title: "Choosing Decision Thresholds for Bayesian Sequential Testing"
author: "Fan Bu"
date: "04/15/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r source functions, echo=FALSE, message=FALSE, warning=FALSE}
source('simpleCalibration.R')

```


## Problem setup

We wish to test these hypotheses regarding (log) effect $\beta$:
$$
H_0: \beta \leq h \quad \text{  v.s. } \quad H_1: \beta > h.
$$
Usually $h=0$, but we may want to change the hypothesis threshold $h$ value for "calibration" purposes as $\beta$ may be a biased estimand (which is what FDA usually does). 

What actually happens in our Bayesian sequential test, given a decision threshold $\delta_1$ on the posterior probability:

1. For analysis time point $t=1:T$, compute posterior probability for $H_1$ for each negative control outcome $o$:
$$
p_{1t,o} = P(\beta_t > h \mid \text{data at time } t).
$$

2. Across all time points $t$, $H_1$ will be claimed true for outcome $o$ if $\max_t(p_{1t,o}) > \delta_1$.

3. Type I error rate $e_1$ is estimated as the proportion among all outcome $o$'s for which $\max_t(p_{1t,o}) > \delta_1$ (i.e., a positive signal is claimed).


## How to choose proper thresholds

Domain experts may have direct knowledge about appropriate values for the hypothesis threshold $h$ and the probability threshold $\delta_1$. However, when we don't have such domain expertise, we need to choose reasonable thresholds, perhaps according to certain operating characteristics. 

Suppose that we want to the limit Type I error rate such that $e_1 \leq \alpha$ --- or, practically, we would prefer that $e_1 \approx \alpha$. Then potentially we can make choices about:

1. Hypothesis threshold $h$ --- i.e., we may change the hypotheses we practically test to correct for systematic error in order to test the hypotheses we actually want

2. Probability threshold $\delta_1$ --- i.e., we may change the Bayesian decision rule to attain a desired overall (Type I) error rate


Furthermore, we also have systematic error distributions estimated from negative control analyses, and so we can de-bias (or "adjust") the posterior samples of $\beta$ accordingly. 
Thus, there are in fact **four** variations we can potentially do:

1. Choose hypothesis threshold $h$, **with** de-biasing/adjustment

2. Choose hypothesis threshold $h$, **without** de-biasing/adjustment

3. Choose probability threshold $\delta_1$, **with** de-biasing/adjustment

4. Choose probability threshold $\delta_1$, **without** de-biasing/adjustment


**Remark**. Choosing proper thresholds is similar, in principle, to empirical calibration. However, here we operate under the Bayesian framework so we technically have two types of "critical values" that we can adjust in order to achieve desired test performance. 


### Brief description: how to choose hypothesis threshold $h$

1. Pre-specify a reasonable value for $\delta_1$ (e.g., $0.95$) and $\alpha$ (e.g., $0.05$)

2. Access all adjusted/unadjusted posterior samples of $\beta$ for all negative control outcomes (for a specific analysis design and exposure etc.)

3. Use binary search (within a pre-specified search range) to find a proper $h$ value such that $e_1$ (estimated Type I error rate for these outcomes) gets very close to $\alpha$ (more details in appendix)

4. Retain the $h$ value and do further evaluation (e.g., Type 2 error rate) on it

### Brief description: how to choose probability threshold  $\delta_1$

1. Set $h=0$ (probably much more reasonable if using de-biased/adjusted estimates) and choose $\alpha$ (e.g., $0.05$)

2. Find $mp_{o} = \max_t(p_{1t,o})$'s for all the negative control outcomes of interest (for a specific analysis design and exposure etc.)

3. Set $\delta_1$ as the $1-\alpha$ quantile of all $mp_{o}$ values

4. Retain the $\delta_1$ value and do further evaluation (e.g., Type 2 error rate) on it

### Metrics

1. Type I error (error rates on negative controls)

2. Type II error (error rates on positive controls)

<!-- 3. F1 score -->
<!-- $$ -->
<!-- F_1 = \frac{2}{1/P + 1/R} = \frac{2 P \times R}{P + R}, -->
<!-- $$ -->
<!-- where $P = \frac{TP}{TP+FP}$ precision, $R = 1 - \text{type 2 error}$ recall.  -->


## Examples


```{r paths and other setup, echo=FALSE}
summarypath = '~/Documents/Research/betterResults/summary'
samplepath = "/Volumes/WD-Drive/betterResults-likelihoodProfiles/"
cachepath = '../localCache/'

db = 'CCAE'
eid = 211981
aid = 2
pid = 3 # 1: sd=10; 2: sd=1.5; 3: sd=4 
tolerance = 0.004

```

<!-- #### Example 1: without de-biasing/adjustment, `Historical Comparator` design -->

#### Example 1: `Historical Comparator` design, with or without de-biasing/adjustment

```{r ex1, results='hide', cache=TRUE}
res1 = plotCalibration(database_id = db,
                method = 'HistoricalComparator', # 'SCCS'
                analysis_id = aid,
                exposure_id = eid,
                prior_id = pid,
                summaryPath = summarypath,
                samplePath = samplepath,
                cachePath = cachepath,
                tol = tolerance,
                useAdjusted = list(delta1 = FALSE, null=FALSE), 
                showPlots = FALSE)
```

<!-- #### Example 2: with de-biasing/adjustment, `Historical Comparator` design -->

```{r ex2, results='hide',cache=TRUE}
res2 = plotCalibration(database_id = db,
                method = 'HistoricalComparator', # 'SCCS'
                analysis_id = aid,
                exposure_id = eid,
                prior_id = pid,
                summaryPath = summarypath,
                samplePath = samplepath,
                cachePath = cachepath,
                tol = tolerance,
                useAdjusted = list(delta1 = TRUE, null=TRUE),
                showPlots = FALSE)
```

```{r ex1 and ex2 errors, fig.width=13, fig.height=5}
plotSideBySide(res2, res1, plotToShow = 'error')
```

<!-- # ```{r ex1 and ex2 f1 scores, fig.width=7, fig.height=4} -->
<!-- # plotSideBySide(res2, res1, plotToShow = 'f1') -->
<!-- # ``` -->



<!-- #### Example 3: with de-biasing/adjustment, `SCCS` design -->
#### Example 2: `SCCS` design, with or without de-biasing/adjustment

```{r ex3, results='hide',cache=TRUE}
res3 = plotCalibration(database_id = db,
                method = 'SCCS',
                analysis_id = aid,
                exposure_id = eid,
                prior_id = pid,
                summaryPath = summarypath,
                samplePath = samplepath,
                cachePath = cachepath,
                tol = tolerance,
                useAdjusted = list(delta1 = TRUE, null=TRUE),
                showPlots = FALSE)
```

<!-- #### Example 4: without de-biasing/adjustment, `SCCS` design -->
```{r ex4, results='hide',cache=TRUE}
res4 = plotCalibration(database_id = db,
                method = 'SCCS',
                analysis_id = aid,
                exposure_id = eid,
                prior_id = pid,
                summaryPath = summarypath,
                samplePath = samplepath,
                cachePath = cachepath,
                tol = tolerance,
                useAdjusted = list(delta1 = FALSE, null=FALSE),
                showPlots = FALSE)
```

```{r ex3 and ex4 errors, fig.width=13, fig.height=5}
plotSideBySide(res3, res4, plotToShow = 'error')
```

<!-- # ```{r ex3 and ex4 f1 scores, fig.width=7, fig.height=4} -->
<!-- # plotSideBySide(res3, res4, plotToShow = 'f1') -->
<!-- # ``` -->

Some findings from these examples:

1. `SCCS` makes less false positives even without bias adjustment.

2. Type I error rates are lowered with bias adjustment, and "calibrating" on thresholds offers an "additional fix".

3. (More prominent for Historical Comparator) Bias adjustment + "calibrating" on $\delta_1$ achieves desired Type I error rate while sacrificing the least on power (least Type II error rate).

#### Estimation biases without adjustment

<!-- Below we plot the distributions of unadjusted negative control estimates (using MAP estimates) for Historical Comparator and SCCS designs (in Examples 1 and 2).  -->

```{r get biases, cache=TRUE, include=FALSE}
HCbiases1 = getBiases(database_id = db, method = 'HistoricalComparator',
                     exposure_id = eid, analysis_id = aid, prior_id = pid,
                     resPath = summarypath, estimateType = 'MAP')

SCCSbiases1 = getBiases(database_id = db, method = 'SCCS',
                     exposure_id = eid, analysis_id = aid, prior_id = pid,
                     resPath = summarypath, estimateType = 'MAP')

dat1 = bind_rows(as.data.frame(HCbiases1), as.data.frame(SCCSbiases1)) %>% 
  select(outcome_id, estimates, method, mean, sd, num) %>%
  mutate(label = sprintf('Num. of NCs: %s\nMean: %.3f\nStd: %.3f',
                         num, mean, sd),
         method = if_else(method == 'HistoricalComparator', 
                          'Historical Comparator', 'SCCS'))

```


```{r plot bias distributions, include=FALSE, fig.width=4, fig.height=3}
ggplot(dat1, aes(x=estimates)) +
  geom_density(aes(fill = method), alpha = 0.8) +
  facet_grid(method~.)+
  geom_text(x=1, y = 0.6, aes(label = label), 
            fontface = 'plain', hjust = 0) +
  scale_fill_manual(values = wes_palette("Chevalier1")[2:3]) +
  labs(x='Bayesian estimates for negative control outcomes',
       fill = '', y = '') +
  theme_bw(base_size = 14) +
  theme(legend.position = 'bottom')
```

```{r get biases from EUMAEUS, cache=TRUE}
localCache = '../localCache'
HCbiases2 = getBiases(database_id = db, method = 'HistoricalComparator',
                     exposure_id = eid, analysis_id = aid, prior_id = pid,
                     resPath = localCache, source='EUMAEUS')

SCCSbiases2 = getBiases(database_id = db, method = 'SCCS',
                     exposure_id = eid, analysis_id = aid, prior_id = pid,
                     resPath = localCache, source='EUMAEUS')

# dat2 = bind_rows(as.data.frame(HCbiases2), as.data.frame(SCCSbiases2)) %>% 
#   select(outcome_id, estimates, method, mean, sd, num) %>%
#   mutate(label = sprintf('Num. of NCs: %s\nMean: %.3f\nStd: %.3f',
#                          num, mean, sd),
#          method = if_else(method == 'HistoricalComparator', 
#                           'Historical Comparator', 'SCCS'))

```

```{r plot bias distributions from EUMAEUS, fig.width=8, fig.height=6, eval=FALSE}
ggplot(dat2, aes(x=estimates)) +
  geom_density(aes(fill = method), alpha = 0.8) +
  facet_grid(method~.)+
  geom_text(x=1, y = 0.6, aes(label = label), 
            fontface = 'plain', hjust = 0) +
  scale_fill_manual(values = wes_palette("Chevalier1")[2:3]) +
  labs(x='Estimates for negative control outcomes',
       fill = '', y = '') +
  theme_bw(base_size = 14) +
  theme(legend.position = 'bottom')
```

Below we also plot the negative control estimates (without bias adjustment) for the two examples above to show the scale of biases. 
We do see slightly less bias and better CI coverage with `SCCS`, although in both cases there isn't substantial bias. 

```{r funnel plots instead, message=FALSE, warning=FALSE, fig.width=10, fig.height=4.5}
HCplot = plotSystematicErrors(HCbiases2) + labs(title='Historical Comparator')
SCCSplot = plotSystematicErrors(SCCSbiases2) + labs(title='SCCS')
ggpubr::ggarrange(HCplot, SCCSplot,
                  # labels = c('Historical Comparator', 
                  #            'SCCS'), 
                  ncol = 2)
```


## Discussion

1. How might we justify choosing an $h$ that's non-zero? Note that, in practice, our analysis should be sequential --- is it still a valid test if we practically change the hypotheses at every timepoint of testing? 


2. How to interpret Bayesian estimates (with/without adjustments) after the "calibration"?

   **Our proposal**: Keep $h=0$, do bias adjustment, and then choose probability threshold $\delta_1$ on operation characteristics (e.g., Type I error); then the posterior distribution *after* bias adjustment does produce interpretable estimates. The unbiasedness of the de-biased estimates can be easily checked empirically, though we do need some (theoretical or empirical) argument for maintaining $h=0$ but selecting $\delta_1$ to achieve good testing performance. 

3. This whole procedure is retrospective --- how to do it in a properly sequential way?

    **One possibility**: 
  
    i. At each timepoint $t$, select (or re-select) proper thresholds using the above-described procedure, *given data accrued up to time $t$*
  
    ii. In this way, we can at least ensure the performance (e.g., not too many false positives) *up to time $t$*
  
    iii. However, we can't say much about what would happen in the future, if we don't want to pre-fix the testing schedule
  
    iv. We also don't know what to do if a signal shouldn't be claimed given the "calibrated" thresholds at time $t$ **but** it was already claimed as a signal given the previously chosen thresholds at, say, time $t-1$
  
    **Another possibility**:
  
    i. At each timepoint $t$, given all information accrued, simulate/sample the posterior samples and posterior probabilities for the negative controls for all the future analysis time steps
  
    ii. Based on the simulated results, choose proper thresholds to maintain desired error rates (e.g., Type I error rate)
  
    iii. *Major problem* with this approach: it still suffers the fixed schedule of MaxSPRT, but we want to have more flexibility


-------------------


### Appendix

#### A1. Calibrating $h$: more details

We can show that, given a decision threshold $\delta_1$, the (empirical) Type I error rate ($O$ is the total number of negative controls)
$$
e_{1}(h) = \frac{1}{O} \sum_{o=1}^O I(\max_{t}(p_{1t,o}) > \delta_1)
$$
is a decreasing step function of $h$, with $e_1(h) \rightarrow 1$ as $h \rightarrow -\infty$ and $e_1(h) \rightarrow 0$ as $h \rightarrow \infty$. A rough illustration is shown below.
```{r demo plot,fig.width=5, fig.height=3.5, fig.align='center'}
type1func <- function(x, knots){
  mean(knots > x)
}

kts = runif(50, min = -1, max = 3)
x = seq(from = -1.3, to = 3.3, by = 0.005)
y = sapply(x, type1func, knots = kts)

dat = data.frame(x=x, y=y)

ggplot(dat, aes(x=x,y=y)) +
  geom_line() +
  geom_hline(yintercept = 0.05, color="#547BD3", size = 0.8)+
  #scale_y_continuous(breaks = c(0,0.05, 1))+
  labs(y='Type I error', x='h') +
  theme_bw(base_size = 14)

```

Therefore, for a desired Type I error rate level $\alpha$ (e.g., $\alpha = 0.05$) given a search range $[a,b]$ we can use binary search to find a value $h^*$ such that $\lvert e_1(h^*) - \alpha \rvert < \epsilon$ for a specified tolerance $\epsilon$. 


