---
title: "| RESEARCH PROTOCOL\n| \n| BETTER: Bayesian Evaluation of Time-To-Event and Reliability (for vaccine surveillance)\n"
fontsize: 12pt
geometry: margin=1in
output:
  bookdown::html_document2:
    df_print: paged
    toc: yes
    toc_depth: 2
    toc_float: yes
    number_sections: yes
    number_tables: yes
    css: "style.css"
  bookdown::word_document2:
    toc: yes
  bookdown::pdf_document2:
    keep_tex: yes
    latex_engine: xelatex
    md_extensions: +raw_attribute
    number_sections: yes
    # citation_package: natbib
    includes:
      before_body: title.tex
header-includes:
- \usepackage[numbers,sort&compress]{natbib}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{caption}
- \usepackage{rotating}
- \usepackage{multirow}
- \usepackage{mwe,tikz}
- \usepackage[percent]{overpic}
- \usepackage{enumitem}
- \usepackage{hyperref}
- \newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}
- \newcommand{\footerDate}{`r params$date`}
- \input{header.tex}
longtable: yes
mainfont: Arial
bibliography: Protocol.bib
params:
  date: '2021-12-20'
  version: 1.0.1
subtitle: 'Version: `r params$version`'
link-citations: true
csl: jamia.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
options(kableExtra.latex.load_packages = FALSE)
library(kableExtra)
#knitr::knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage[table]{xcolor}', x, fixed = TRUE)})
library(dplyr)
options(knitr.kable.NA = "")
if (!knitr::is_latex_output() && !knitr::is_html_output()) {
  options(knitr.table.format = "simple")
}

pdf2png <- function(path) {
  # only do the conversion for non-LaTeX output
  if (knitr::is_latex_output()) {
    return(path)
  }
  path2 <- xfun::with_ext(path, "png")
  img <- magick::image_read_pdf(path)
  magick::image_write(img, path2, format = "png")
  path2
}


latex_table_font_size <- 8

source("PrintCohortDefinitions.R")

numberOfNcs <- nrow(readr::read_csv("../inst/settings/NegativeControls.csv", col_types = readr::cols()))

```

# List of Abbreviations

```{r abbreviations, echo=FALSE, results="asis", warning=FALSE}
abbreviations <- readr::read_delim(col_names = FALSE, delim = ";", trim_ws = TRUE, file = "
  AUC; Area Under the receiver-operator Curve 
  CCAE; IBM MarketScan Commercial Claims and Encounters 
  CDM; Common Data Model
  CIOMS; Council for International Organizations of Medical Sciences
  COVID-19; COronaVIrus Disease 2019
  CRAN; Comprehensive R Archive Network
  EHR; Electronic Health Record
  H1N1pdm; Hemagglutinin Type 1 and Neuraminidase Type 1 (2009 pandemic influenza)
  HPV; Human PapillomaVirus
  IRB; Institutional review board
  LLR; Log Likelihood Ratio
  MAP; Maximum A Posteriori
  MaxSPRT; MAXimized Sequential Probability Ratio Test 
  MCMC; Markov Chain Monte Carlo
  MDCR; IBM MarketScan Medicare Supplemental Database
  MDCD; IBM MarketScan Multi-State Medicaid Database 
  MSE; Mean Squared Error
  OHDSI; Observational Health Data Science and Informatics
  OMOP; Observational Medical Outcomes Partnership
  RCT; Randomized controlled trial
  SCCS; Self-Controlled Case Series
  SCRI; Self-Controlled Risk Interval
  WHO; World Health Organization
")

tab <- kable(abbreviations, col.names = NULL, linesep = "", booktabs = TRUE)

if (knitr::is_latex_output()) {
  tab %>% kable_styling(latex_options = "striped", font_size = latex_table_font_size)
} else {
  tab %>% kable_styling(bootstrap_options = "striped")
} 
```

# Responsible Parties

## Investigators

```{r parties, echo=FALSE, results="asis", warning=FALSE}
parties <- readr::read_delim(col_names = TRUE, delim = ";", trim_ws = TRUE, file = "
  Investigator; Institution/Affiliation
  Fan Bu *; Department of Human Genetics, University of California, Los Angeles, Los Angeles, CA, USA
  George Hripcsak; Department of Biomedical Informatics, Columbia University, New York, NY, USA
  Patrick B. Ryan; Observational Health Data Analytics, Janssen Research and Development, Titusville, NJ, USA
  Marc A. Suchard; Department of Biostatistics, University of California, Los Angeles, Los Angeles, CA, USA
")

tab <- kable(parties, booktabs = TRUE, linesep = "") %>% 
  column_spec(1, width = "10em") %>%
  column_spec(2, width = "30em") %>%
  footnote(general = "* Principal Investigator", general_title = "")

if (knitr::is_latex_output()) {
  tab %>% kable_styling(latex_options = "striped", font_size = latex_table_font_size)
} else {
  tab %>% kable_styling(bootstrap_options = "striped")
}
```

## Disclosures

This study is undertaken within Observational Health Data Sciences and Informatics (OHDSI), an open collaboration.
**GH** receives grant funding from the US National Institutes of Health and the US Food & Drug Administration.
**PBR** and is an employee of Janssen Research and Development and shareholders in John & Johnson.
**FB** and **MAS** receive grant funding from the US National Institutes of Health and the US Food & Drug Administration and contracts from the US Department of Veterans Affairs and Janssen Research and Development.

# Abstract

**Background and Significance**

<!-- As recently approved COVID-19 vaccines are rolled out globally, it is likely that safety signals will be identified from spontaneous reports and other data sources. Although some work has been done on the best methods for vaccine safety surveillance, there is a scarcity of information on how these perform in analyses of real-world data. -->

As various approved COVID-19 vaccines are rolled out globally, safety signals have been identified from spontaneous reports and other data sources.
The current standard method of safety surveillance adopted by the FDA is MaxSPRT, which suffers from the inflexibility of a pre-specified sequential analysis schedule. 
We hope to develop and implement a more flexible Bayesian surveillance framework and compare its performance with MaxSPRT in real-world data. 

**Study Aims**

To compare the real-data performance (testing errors, timeliness,  precision and bias) of
Bayesian and frequentist sequential analysis methods for the study of comparative vaccine safety.

We will also produce a reference table of Type I and II error rates and signal detection times for all combinations of design and threshold choices, as exploration of the operating characteristics of Bayesian sequential methods.  

**Study Description**

* Design: historical comparator & self-controlled studies
* Exposures: previous viral vaccines including 2017-2018 flu, H1N1pdm flu, Human Papillomavirus (HPV), and Varicella-Zoster.
* Outcomes: selected adverse events of special interest (e.g., **Guillain-Barre Syndrome**); negative control outcomes; imputed positive control outcomes
* Analyses: 
    1) historical comparator/historical rates (frequentist)
    2) historical comparator/historical rates (Bayesian)
    3) Self-controlled case series with variations (frequentist)
    4) Self-controlled case series with variations (Bayesian)
* Decision rules:
    - Frequentist method: reject null at $\alpha = 0.05$ level, using the MaxSPRT adjustment 
    - Bayesian method:
        + Posterior probability of _signal_, $P_1 = P(H_1 \text{ true, signal} \mid \text{data})$; reject null (claim signal) when $P_1 > \delta_1$, with $\delta_1 = 0.80, 0.90, 0.95$;
        + Posterior probability of _futility_, $P_0 = P(H_0 \text{ true, safety}\mid \text{data})$; accept null (claim safety) when $P_0 > \delta_0$, with $\delta_0 = 0.90, 0.95, 0.99$.
* Metrics:

    1. Hypothesis testing related metrics
    
    - Type 1 error. For negative controls, how often was the null rejected using the various decision rules. This is equivalent to the false positive rate and 1 - specificity.
    - Type 2 error. For positive controls, how often was the null **not** rejected using the various decision rules. This is equivalent to the false negative rate and 1 - sensitivity. Will be stratified by true effect size of the positive controls.
    - Posterior probability of futility ($H_0$ true) at final analysis; only reported for Bayesian methods. 
    - Sensitivity and specificity based on the various decision rules
    - Detection time: the number of analyses (months) until signals are claimed for 80% of positive controls. Will be stratified by true effect size of the positive controls.
    - Rate of contradictory early decisions. For all controls, how often did an earlier signal/futility decision contradict the decision based on full analysis of all data. 
    - Rate of "undetermined'' decisions. At each analysis stage, for all controls, how often are the decisions "undetermined" (i.e., neither decision threshold is crossed). For most analyses, we would expect this rate to be high at earlier stages when there isn't enough data evidence but gradually lower as more data accrue. 
    <br>
    <br>
    2. Estimation related metrics
    
    - Area Under the receiver-operator Curve (AUC). The ability to discriminate between positive controls and negative controls based on the point estimate of the effect size. Will be stratified by true effect size of the positive controls.
    - Coverage. How often the true effect size is within the 95% confidence (or credible) interval.
    - Mean precision, computed as $1 / (\text{standard error})^2$ (for the Bayesian method, ``standard error'' is taken as the square root of the posterior distribution variance)
    - Mean squared error (MSE). Mean squared error between the log of the effect size point-estimate (MAP estimate for Bayesian method) and the log of the true effect size.
    - Non-estimable. Measure for how many of the controls was the method unable to produce an estimate
    

# Amendments and Updates

Table \@ref(tab:amendments) lists any protocol amendments made over time.

```{r amendments, echo=FALSE, results="asis", warning=FALSE}
amendments <- readr::read_delim(col_names = TRUE, delim = ";", trim_ws = TRUE, file = "
  Number; Date; Section of study protocol; Amendment or update; Reason
")

tab <- kable(amendments, booktabs = TRUE, linesep = "", caption = "Protocol amendments")

if (knitr::is_latex_output()) {
  tab %>%
    column_spec(1, width = "4em") %>%
    column_spec(2, width = "6em") %>%
    column_spec(3, width = "10em") %>%
    column_spec(4, width = "15em") %>%
    column_spec(5, width = "15em") %>%
    kable_styling(latex_options = "striped", font_size = latex_table_font_size)

} else {
  tab %>% kable_styling(bootstrap_options = "striped")
}
```


# Milestones


Table \@ref(tab:dates) lists the study milestones.

```{r dates, echo=FALSE, results="asis", warning=FALSE}
dates <- readr::read_delim(col_names = TRUE, delim = ";", trim_ws = TRUE, file = "
  Milestone; Planned / actual date
  Start of analysis; 02/01/2022
  End of analysis; 05/31/2022
  Results presentation; 06/30/2022
")

tab <- kable(dates, booktabs = TRUE, linesep = "", caption = "Study milestones")

if (knitr::is_latex_output()) {
  tab %>% kable_styling(latex_options = "striped", font_size = latex_table_font_size)
} else {
  tab %>% kable_styling(bootstrap_options = "striped")
}
```

# Rationale and Background

Mass vaccination against SARS-CoV-2 is critical to ending the current COVID-19 global pandemic. 
By the beginning of 2022, 9 vaccines have been approved under the WHO Emergency Use List, and more than 10 billion doses have been administered globally by February 2022 [@WHOdashboard2022].
With the large-scale usage of vaccines under emergency approval, it is essential to ensure their safety and effectiveness through post-market surveillance, as rare but serious adverse events may not be identified in phase 3 clinical trials. 
In the US, messenger RNA (mRNA) vaccines (BNT162b2, Pfizer-BioNTech; and mRNA-1273, Moderna) were the first SARS-CoV-2 vaccines authorized and as of February 2022, more than 500 million doses of mRNA vaccines have been administered. [@Statista2022]
And yet, there is limited experience with mRNA platforms previous to SARS-CoV-2, and therefore safety surveillance is particularly important to inform public health policy and maintain public trust. 

The design of a rapid and reliable vaccine safety surveillance system requires an efficient and robust statistical monitoring approach. 
The current standard approach used by regulatory agencies in the US is a frequentist sequential analysis method, MaxSPRT [@kulldorf2011]. 
It is designed to control the overall analysis Type I error rate of a sequential analysis by allocating the allowed false positive error over sequential analysis stages. 
This method has long suffered from its inflexibility as it requires a pre-specified analysis schedule, and does _not_ allow extended analysis after the pre-chosen analysis endpoint. 

A more flexible sequential analysis method is, therefore, much desired. 
A promising candidate is a Bayesian sequential testing framework.
Under a Bayesian framework, multiplicity of sequential analyses can be handled more elegantly, without the need for a rigid, pre-specified analysis schedule while allowing continued analyses beyond anticipated endpoints. It is also easier to incorporate historical information into current analyses using a prior distribution through Bayesian inference. 
With all its theoretical advantages, however, the performance and operating characteristics of Bayesian sequential testing methods have not yet been extensively studied on large-scale real-world data. 
In particular, with observational health data, Bayesian methods can potentially adjust for unmeasured confounding and sampling bias, but the performance and behavior have not been evaluated in a systematic manner.

The goal of this study is to compare the performance of a Bayesian testing framework with that of MaxSPRT (the current standard approach), in terms of both the hypothesis testing errors (sensitivity and specificity) and estimation accuracy (in estimating the relative risks of adverse events of interest).
This study will be conducted on various large-scale health claims databases, in order to understand the operating characteristics in a real-world data-intensive setting. 
At the initial stage of the study, all analyses will be performed _retrospectively_ using historical vaccines with more regular roll-out schedules. 
We believe the results of our comprehensive evaluations will help us better understand the performance and behavior of a Bayesian sequential testing framework and facilitate the design of a more flexible and reliable safety surveillance system for COVID-19 vaccines. 

# Study Objectives

The overarching aim is to compare the performance of frequentist and Bayesian sequential analysis methods for the generation of evidence of vaccine safety in observational, real-world data.
Specific aims:

- To evaluate and compare the operating characteristics (Type I and II errors, sensitivity and specificity, etc.) of frequentist and Bayesian sequential testing methods
- To compare the 'timeliness' of these methods for the identification of vaccine safety signals
- To estimate the bias and precision associated with the use of frequentist and Bayesian methods with self-controlled or historical rates designs for the study of vaccine safety
- To gain a deeper understanding of the behavior of Bayesian sequential methods; specifically, the relationship between threshold choices, Type I and II errors, and time-to-signal -- the study will produce **a reference table of estimated Type I and II errors and time-to-signal** for each combination of Bayesian sequential testing choices


# Research Methods

## Exposure-outcome pairs

### Exposures

The evaluation will center on six existing (groups of) vaccines, for specific time periods (start date to end date), as shown in Table \@ref(tab:exposures-of-interest).

```{r exposures-of-interest, echo=FALSE, warning=FALSE}
eois <- readr::read_csv(system.file("settings", "ExposuresOfInterest.csv", package = "better"), col_types = readr::cols()) %>%
  select(exposureName, startDate, endDate, historyStartDate, historyEndDate)
colnames(eois) <- SqlRender::camelCaseToTitleCase(colnames(eois))

tab <- eois %>%
  kable(booktabs = TRUE, linesep = "",
      caption = "Exposures of interest.") %>% 
  kable_styling(bootstrap_options = "striped", latex_options = "striped")

if (knitr::is_latex_output()) {
  tab %>%
    column_spec(1, width = "18em") %>%
    column_spec(2, width = "8em") %>%
    column_spec(3, width = "8em") %>%
    column_spec(4, width = "8em") %>%
    column_spec(5, width = "8em") %>%
    kable_styling(font_size = latex_table_font_size)
} else {
  tab
}
```

For some methods the period between historical start and historical end date will be used to estimate the historic incidence rate. 
For analyses executed on data in the southern hemisphere (if any) the flu seasons are different, and the study periods will need to be adjusted accordingly.
The formal cohort definitions of each exposure can be found in Appendix \@ref(exposure-cohort-definitions). 

### Negative control outcomes 

Negative controls are outcomes believed not to be caused by any of the vaccines, and therefore ideally would not be flagged as a signal by a safety surveillance system. 
Any effect size estimates for negative control ideally should be close to the null. 

A single set of negative control outcomes is defined for all four vaccine groups.
To identify negative control outcomes that match the severity and prevalence of suspected vaccine adverse effects, a candidate list of negative controls was generated based on similarity of prevalence and percent of diagnoses that were recorded in an inpatient setting (as a proxy for severity). 
Manual review of this list by clinical experts created the final list of `r numberOfNcs` negative control outcomes. 
The full list of negative control outcomes can be found in Appendix \@ref(negative-controls)

Negative control outcomes are defined as the first occurrence of the negative control concept or any of its descendants. 

### Imputed positive control outcomes 

Positive controls are outcomes known to be caused by vaccines, and ideally would be detected as signals by a safety surveillance system as early as possible.
For various reasons, real positive controls are problematic.[@Schuemie2018-zi] 
Instead, here we will rely on imputed positive controls, created by shifting the estimated effect sizes for the negative controls. 
We assume the negative controls have a true effect size of 1, so to simulate the estimated effect size when the true effect size is $\theta$ we multiply the estimate by $\theta$. 
For example, if for a negative control a method produces an effect size estimate of 1.1, for a positive control with true effect size of 2 the estimated effect size becomes 1.1 x 2 = 2.2. 
This approach makes strong assumptions on the nature of the systematic error, most importantly that systematic error does not change as a function of the true effect size. 
Although this assumption is likely not to hold in the real world, imputing positive controls allows us to provide some indication of what type 2 error to expect for various true effect sizes. 
For each negative control we will impute positive controls with true effect sizes of 1.5, 2, and 4, so using the `r numberOfNcs` negative controls we are able to construct `r numberOfNcs` $\times$ 3 = `r numberOfNcs*3` positive control outcomes.
This increased true effect is applied both for the first and second injection of multi-dose vaccines.

### Outcome of special interest --- Guillain-Barre Syndrome

In addition to the negative control and imputed positive control outcomes, we will further investigate the risk of Guillain-Barre Syndrome (GBS) following the zoster vaccine, as comparison to previous study findings [@goud2021risk]. 
The previous study by Goud et al. used the self-controlled case series design to analyze Medicare claims data, and found a significant elevated risk (risk ratio 2.34, 95% CI, 1.01-5.41).
We will use both the historical comparator and self-controlled designs, apply both frequentist and Bayesian sequential testing methods, and run analyses on a variety of large-scale databases, in the hope of a more comprehensive analysis of the risk of GBS post zoster vaccination. 

## Data sources

We will execute BETTER as an OHDSI network study.
All data partners within OHDSI are encouraged to participate voluntarily and can do so conveniently, because of the community's shared Observational Medical Outcomes Partnership (OMOP) common data model (CDM) and OHDSI tool-stack.
Many OHDSI community data partners have already committed to participate and we will recruit further data partners through OHDSI’s standard recruitment process, which includes protocol publication on OHDSI’s GitHub, an announcement in OHDSI’s research forum, presentation at the weekly OHDSI all-hands-on meeting and direct requests to data holders.

Table \@ref(tab:data-sources) lists the potential data sources for BETTER; these sources encompass a large variety of practice types and populations. 
For each data source, we report a brief description and size of the population it represents.
All data sources will receive institutional review board approval or exemption for their participation before executing BETTER.

```{r data-sources, echo=FALSE, warning=FALSE}
data_sources <- readr::read_delim(col_names = TRUE, delim = ";", trim_ws = TRUE, file = "
  Data source ; Population ; Patients ; History ; Data capture process and short description
  IBM MarketScan Commercial Claims and Encounters (CCAE) ; Commercially insured, < 65 years ; 142M ; 2000 -- ; Adjudicated health insurance claims (e.g. inpatient, outpatient, and outpatient pharmacy)  from large employers and health plans who provide private healthcare coverage to employees, their spouses and dependents.
  IBM MarketScan Medicare Supplemental Database (MDCR)  ; Commercially insured, 65$+$ years ; 10M ; 2000 -- ; Adjudicated health insurance claims of retirees with primary or Medicare supplemental coverage through privately insured fee-for-service, point-of-service or capitated health plans.
  IBM MarketScan Multi-State Medicaid Database (MDCD) ; Medicaid enrollees, racially diverse ; 26M ; 2006 -- ; Adjudicated health insurance claims for Medicaid enrollees from multiple states and includes hospital discharge diagnoses, outpatient diagnoses and procedures, and outpatient pharmacy claims.
  Optum Clinformatics Data Mart (Optum) ; Commercially or Medicare insured ; 85M ; 2000 -- ; Inpatient and outpatient healthcare insurance claims.
  Optum Electronic Health Records (OptumEHR) ; US, general ; 93M ; 2006 -- ; Clinical information, prescriptions, lab results, vital signs, body measurements, diagnoses and procedures derived from clinical notes using natural language processing. 
")
tab <- kable(data_sources, booktabs = TRUE, linesep = "",
      caption = "BETTER data sources and the populations they cover.") %>% 
  kable_styling(bootstrap_options = "striped", latex_options = "striped") #%>%
  # pack_rows("Administrative claims", 1, 4, latex_align = "c", indent = FALSE) %>%
  # pack_rows("Electronic health records (EHRs)", 5, 5, latex_align = "c", indent = FALSE)

if (knitr::is_latex_output()) {
  tab %>%
    column_spec(1, width = "10em") %>%
    column_spec(2, width = "10em") %>%
    column_spec(5, width = "10em") %>%
    kable_styling(font_size = latex_table_font_size)
} else {
  tab
}
```

## Methods to evaluate

Vaccine safety surveillance methods can be broken down into four components: construction of a *counterfactual* (often referred to as the 'expected count'), a *time-at-risk*, the estimation outcome (an *estimate* or *posterior distribution* for the effect size), and a *decision rule* based on the estimation outcome to differentiate signals from non-signals.

### Counterfactual construction

In this study, we mainly focus on two designs for counterfactual construction: historical comparator and self-controlled case series. 
The former design is currently the standard design adopted by various regulatory agencies such as FDA and CDC in the US and the CDC in the EU, while the latter is a design of rising popularity that has shown satisfactory performance according to numerous recent studies. 

#### Historical Comparator (HC)

Traditionally, vaccine surveillance methods compute an expected count based on an incidence rate estimated during some historic time period, for example, in the years prior to the initiation of the surveillance study. [@li2021characterising][@klein2021surveillance]
We will use the historical period indicated in Table 8.1 and evaluate **four** variations:

- Unadjusted, entire year. Using a single rate computed across the entire historic year for the entire population.
- Age and sex adjusted, entire year. Using a rate stratified by age (in 10 year increments) and sex, computed across the entire historic year. This allows the expected rate to be adjusted for the demographics of the vaccinated.
- Unadjusted, time-at-risk relative to outpatient visit. Using a single rate computed during the time-at-risk relative to a random outpatient visit in the historic year.
- Age and sex adjusted, time-at-risk relative to outpatient visit. Using a rate stratified by age and sex, computed during the time-at-risk relative to a random outpatient visit in the historic year.

<!-- Results in the previous EUMAEUS study show that this counterfactual approach is sensitive to changes in coding practices. Following our previous study, we introduce a study diagnostic: the percent change in overall incidence rate (across the entire population) between the historic and current time period.  -->
<!-- For each of the four variations listed above, we add a new variation where effect-size estimates are removed if the change in incidence rate is greater than 50%. -->


#### Self-Controlled Case Series (SCCS)/Self-Controlled Risk Interval (SCRI)

The SCCS and SCRI designs are self-controlled, comparing the time-at-risk (the time shortly following the vaccination) to some other time in the same patient's record. 
The SCCS design uses all patient time when not at risk as the control time. [@Whitaker2006]
The SCRI design uses a pre-specified control interval relative to the vaccination date as the control time. [@glanz2011]
This unexposed time can be both before or after the time at risk.

We will evaluate **five** variations:

- A simple SCCS, using all patient time when not at risk as the control time, with the exception of the 30 days prior to vaccination which is excluded from the analysis to avoid bias due to contra-indications.
- An SCCS adjusting for age and season. Age and season will be modeled to be constant within each calendar month, and vary across months as bicubic splines.
- A simple SCCS discarding all time prior to vaccination.
- An SCRI, using a control interval of 43 to 15 days prior to vaccination.
- An SCRI, using a control interval of 43 to 71 days after to vaccination.

### Time-at-risk

The time-at-risk is the time window, relative to the vaccination date, when outcomes will potentially be attributed to the vaccine. We define **three** time-at-risk windows: 1-28 days, 1-42 days, and 0-1 days after vaccination. 

Time-at-risk windows will be constructed both for the first and second dose. 
The time-at-risk for one dose will be censored at the time of the next dose. 

### Estimation Outcome

The effect-size of interest for both the HC and SCCS designs is the (log) relative incidence rate ratio.
We obtain slightly different estimation outcomes for the frequentist (i.e., MaxSPRT) and Bayesian methods.

For frequentist MaxSPRT, we obtain:

- Effect-size estimate. This is typically a maximum likelihood estimate (MLE) obtained from the analysis. 

- Log likelihood ratio (LLR). The log of the ratio between the likelihood of the alternative hypothesis (that there is an effect) and the likelihood of the null hypothesis (of no effect).

The LLR is a convenient and commonly used statistic when performing sequential testing, where the LLR can be compared to a pre-computed critical value, as is done in the MaxSPRT method. [@kulldorf2011] 
Although typically MaxSPRT uses a historic rate as counterfactual, any counterfactual can be used to compute the LLR and can be used in MaxSPRT; our use of either the HC or SCCS/SCRI design does not affect the validity of using the LLR as the test statistic. 


For the Bayesian method, we obtain:

- Posterior distribution for the effect-size, approximated by MCMC posterior samples. This is obtained using the Bayes Rule by combining the likelihood function and the prior distribution. The end result is not a single point estimate but rather a *distribution profile* about our knowledge of the effect-size given accrued data. 

- Maximum A Posteriori (MAP) estimate for the effect-size. This is a point estimate obtained by extracting the maxima of the posterior density; this estimate can be regarded as a Bayesian counterpart of the frequentist effect-size estimate. 

- Posterior mean for the effect-size. This is a commonly adopted Bayesian estimate, and is, in fact, the optimal Bayesian estimate with squared loss. 

- Posterior median for the effect-size. This is another commonly adopted Bayesian estimate, and is also the optimal Bayesian estimate with absolute error loss. 

For the Bayesian method, we also evaluate different prior distribution choices for the effect-size:

- A log-normal prior with mean $= 0$ and SD $= 1.5$ (a **conservative** prior with >90% mass below 2)
- A log-normal prior with mean $= 0$ and SD $= 4$ (a **weakly informed** prior with ~70% mass below 2)
- A log-normal prior with mean $= 0$ and SD $= 10$ (a **diffuse** prior)

We choose to use log-normal priors for their simplicity and wide use, in order to focus mainly on comparison between Bayesian and frequentist testing methods.
We will consider adopting other prior distributions (e.g., Laplace priors) in subsequent studies. 


For the frequentist method (MaxSPRT), analyses will be conducted *with* and *without* empirical calibration. [@Schuemie2014-bv;@Schuemie2018-hq]. Empirical calibration will be done using leave-one-out: when calibrating the estimate for a control, the systematic error distribution will be fitted uses all controls except the one being calibrated.

For the Bayesian method, inference will be conducted *with* and *without* Bayesian bias adjustment using negative control analyses. Similarly, bias adjustment will be done using leave-one-out. 

### Decision rule

To identify 'signals' we need a decision rule, for example in the shape of a threshold value on one of the estimates statistics.

In our experiment, for the frequentist surveillance method, we will consider a decision rule using the critical value $cv$ computed for the LLR at the $\alpha = 0.05$ level.
That is, we will reject the null and claim a signal when $LLR > cv$. Here all critical values will be computed using the [`Sequential` package in CRAN](https://cran.r-project.org/web/packages/Sequential/index.html).
<!-- For the historical rates method we will use a Poisson model assuming the counterfactual is known without uncertainty. -->
<!-- For all other methods we will use a binomial model.  -->

For the Bayesian method, we will implement two sets of decision rules, one for signal (rejecting null) and one for futility/safety (accepting null), by examining the posterior probabilities of the null and alternative hypotheses simultaneously:

- If the posterior probability of signal, $P_1 = P(H_1 \text{ true } \mid \text{data}) > \delta_1$, we claim a signal;
- If the posterior probability of futility/safety, $P_0 = P(H_0 \text{ true } \mid \text{data}) > \delta_0$, we claim safety (non-signal). 

We will evaluate three choices of $\delta_1$: $0.80, 0.90, 0.95$; and also three choices of $\delta_0$:$0.90, 0.95, 0.99$.


## Metrics

As we will conduct both estimation and testing tasks at the same time, we will compute two sets of metrics based on the study outcomes: 
(1) metrics for testing, and (2) metrics for estimation. (Some of the following metrics are adapted from previous work [@Schuemie2020-wx].)

1. Testing-related metrics:

- Type 1 error. For negative controls, how often was the null rejected using the various decision rules. This is equivalent to the false positive rate and 1 - specificity.
- Type 2 error. For positive controls, how often was the null **not** rejected using the various decision rules. This is equivalent to the false negative rate and 1 - sensitivity. Will be stratified by true effect size of the positive controls.
- Posterior probability of futility ($H_0$ true) at final analysis; only reported for Bayesian methods. 
- Sensitivity and specificity based on the various decision rules, as well as prior choices in the Bayesian method. 
- Detection time: the number of analyses (months) until signals are claimed for 80% of positive controls. This will be stratified by true effect size of the positive controls.
- Rate of contradictory early decisions. For all controls, how often did an earlier signal/futility decision contradict the decision based on full analysis of all data. This can serve as a measure of temporal stationarity of the sequential process --- if such contradictory rate is high, then there may be time-varying confounding factors left unadjusted for. 
- Rate of "undetermined'' decisions. At each analysis stage, for all controls, how often are the decisions "undetermined" (neither decision thresholds crossed). For most analyses, we would expect this rate to be high at earlier stages when there isn't enough data evidence but gradually lower as more data accrue. 

2. Estimation-related metrics:

- Mean precision, computed as $1 / (\text{standard error})^2$ (for the Bayesian method, ``standard error'' is taken as the square root of the posterior distribution variance)
- Mean squared error (MSE). Mean squared error between the log of the effect size point-estimate (MAP estimate for Bayesian method) and the log of the true effect size.
- Area Under the receiver-operator Curve (AUC). The ability to discriminate between positive controls and negative controls based on the point estimate of the effect size. Will be stratified by true effect size of the positive controls.
- Coverage. How often the true effect size is within the 95% confidence (or credible) interval.
- Non-estimable. Measure for how many of the controls was the method unable to produce an estimate


### Timeliness

To understand the time it takes for a method the identify signals, the study period for each vaccine will be divided into calendar months. 
For each month the methods will be executed using the data that had accumulated up to the end of that month, and the performance metrics will be reported for each month.

### Multiple doses

For those vaccines requiring multiple doses (zoster, HPV), metrics will be computed three times: 

- Treating all doses the same, so computing statistics using both doses without distinguishing between first and second.
- Using the first dose only
- Using the second dose only

## Overview of analyses

In total, we will evaluate:

- 9 counterfactuals
- 3 times at risk (0-1, 1-28, and 1-42 days)
- 6 vaccines, with a total of 9 + 9 + 9 + 9 + 12 + 12 = 60 time periods
- `r numberOfNcs` negative controls
- 3 $\times$ `r numberOfNcs` = `r 3*numberOfNcs` positive controls
- 1 outcome of special interest
- 3 dose definitions (both, first, second) for the zoster and HPV vaccines, 1 for H1N1pdm and seasonal flu.
- 4 prior distribution choices (Bayesian only)
- $3 \times 3 = 9$ decision rule thresholds (Bayesian only)

For the frequentist MaxSPRT method, this will result in a total of 9 $\times$ 3 $\times$ [(9 + 9 + 9 + 9) $\times$ 1 + (12 + 12) $\times$ 3]  $\times$ (`r numberOfNcs` + `r numberOfNcs*3` + 1) = `r format(9 * 3 * ((9 + 9 + 9 + 9) * 1 + (12 + 12) * 3) * (numberOfNcs + (numberOfNcs*3) + 1), scientific = FALSE, big.mark = ",")` effect-size estimates, where each estimate will contain:

- The effect-size estimate with 95% confidence interval and p-value.
- The empirically calibrated effect-size estimate and p-value
- The LLR

For the Bayesian method, this will result in a total of 9 $\times$ 3 $\times$ [(9 + 9 + 9 + 9) $\times$ 1 + (12 + 12) $\times$ 3]  $\times$ (`r numberOfNcs` + `r numberOfNcs*3` + 1) $\times$ 4 = `r format(9 * 3 * ((9 + 9 + 9 + 9) * 1 + (12 + 12) * 3) * (numberOfNcs + (numberOfNcs*3) + 1) * 4, scientific = FALSE, big.mark = ",")` estimation outcomes, where each outcome will include:

- The posterior distribution profile of effect-size with MAP estimate
- The empirically calibrated effect-size posterior distribution profile with MAP estimate
- The decision (signal or safety) made based on the $3 \times 3 = 9$ different decision rule thresholds

These analysis results will be computed for each database.

# Strengths and Limitations

## Strengths

- Use and comprehensive evaluation of Bayesian sequential analysis methods for vaccine safety surveillance on large-scale real-world data.
- Use of self-controlled case series in addition to historical comparator methods, as the former method is less subject to unmeasured confounding and systematic error. 
- Use of real negative and synthetic positive control outcomes provides an independent estimate of residual bias in the experiment.
- The fully specified study protocol is being published before analysis begins.
- Dissemination of the results will not depend on estimated effects, avoiding publication bias.
- All analytic methods have previously been verified on real data.
- All software is freely available as open source.
- Use of a common data model allows extension of the experiment to future databases and allows replication of these results on licensable databases that were used in this experiment, while still maintaining patient privacy on patient-level data.
- Use of multiple databases allows estimating consistency to add credibility and supports generalizability.

 
## Limitations
 
- Even though many potential confounders will be included in this study, there may be residual bias due to unmeasured or misspecified confounders, such as confounding by indication, differences in physician characteristics that may be associated with drug choice, concomitant use of other drugs started after the index date, and informative censoring at the end of the on-treatment periods. To minimize this risk, we used methods to detect residual bias through our negative and positive controls.
- Our follow-up times are limited and variable, potentially reducing power to detect differences in effectiveness and safety.
- We assume hazards are **not** time varying, and we (at this stage) do not investigate time-varying confounding.
- We only adopt two commonly used study designs (at this stage) which may not be the most suitable design for vaccine safety surveillance situations with complex roll-out schedules (e.g., COVID-19 vaccines).
- Misclassification of study variables is unavoidable in secondary use of health data, so it is possible to misclassify treatments, covariates, and outcomes; we do not expect differential misclassification, so bias will most likely be towards the null.

# Protection of Human Subjects

BETTER does not involve human subjects research.
The project does, however, use de-identified human data collected during routine healthcare provision.
All data partners executing the BETTER studies within their data sources will have received institutional review board (IRB) approval or waiver for participation in accordance to their institutional governance prior to execution (see Table \@ref(tab:irb)).
BETTER executes across a federated and distributed data network, where analysis code is sent to participating data partners and only aggregate summary statistics are returned, with no sharing of patient-level data between organizations.

```{r irb, echo=FALSE,warning=FALSE}
data_sources <- readr::read_delim(col_names = TRUE, delim = "&", trim_ws = TRUE, file = "
Data source & Statement
IBM MarketScan Commercial Claims and Encounters (CCAE) & New England Institutional Review Board and was determined to be exempt from broad IRB approval, as this research project did not involve human subject research.
IBM MarketScan Medicare Supplemental Database (MDCR)  & New England Institutional Review Board and was determined to be exempt from broad IRB approval, as this research project did not involve human subject research.
IBM MarketScan Multi-State Medicaid Database (MDCD) & New England Institutional Review Board and was determined to be exempt from broad IRB approval, as this research project did not involve human subject research.
Optum Clinformatics Data Mart (Optum) & New England Institutional Review Board and was determined to be exempt from broad IRB approval, as this research project did not involve human subject research.
Optum Electronic Health Records (OptumEHR) & New England Institutional Review Board and was determined to be exempt from broad IRB approval, as this research project did not involve human subject research.
")

tab <- kable(data_sources, booktabs = TRUE, linesep = "",
      caption = "IRB approval or waiver statement from partners.") %>% 
  kable_styling(bootstrap_options = "striped", latex_options = "striped")

if (knitr::is_latex_output()) {
  tab %>%
    column_spec(1, width = "15em") %>%
    column_spec(2, width = "40em") %>%
    kable_styling(font_size = latex_table_font_size)
} else {
  tab
}
```

# Management and Reporting of Adverse Events and Adverse Reactions

BETTER uses coded data that already exist in electronic databases.
In these types of databases, it is not possible to link (i.e., identify a potential causal association between) a particular product and medical event for any specific individual.
Thus, the minimum criteria for reporting an adverse event (i.e., identifiable patient, identifiable reporter, a suspect product and event) are not available and adverse events are not reportable as individual adverse event reports.
The study results will be assessed for medically important findings.

# Plans for Disseminating and Communicating Study Results

Open science aims to make scientific research, including its data process and software, and its dissemination, through publication and presentation, accessible to all levels of an inquiring society, amateur or professional [@Woelfle2011-ss] and is a governing principle of BETTER.
Open science delivers reproducible, transparent and reliable evidence.
All aspects of BETTER (except private patient data) will be open and we will actively encourage other interested researchers, clinicians and patients to participate.
This differs fundamentally from traditional studies that rarely open their analytic tools or share all result artifacts, and inform the community about hard-to-verify conclusions at completion.

## Transparent and re-usable research tools

We will publicly register this protocol and announce its availability for feedback from stakeholders, the OHDSI community and within clinical professional societies.
This protocol will link to open source code for all steps to generating diagnostics, effect estimates, figures and tables.
Such transparency is possible because we will construct our studies on top of the OHDSI toolstack of open source software tools that are community developed and rigorously tested [@Schuemie2020-wx].
We will publicly host BETTER source code at 
**URL TBD**, allowing public contribution and review, and free re-use for anyone’s future research.

## Continous sharing of results

BETTER embodies a new approach to generating evidence from healthcare data that overcome weaknesses in the current process of answering and publishing (or not) one question at a time.
Generating evidence for thousands of research and control questions using a systematic process enables us to not only evaluate that process and the coherence and consistency of the evidence, but also to avoid $p$-hacking and publication bias [@Schuemie2018-zi].
We will store and openly communicate all of these results as they become available using a user-friendly web-based app that serves up all descriptive statistics, study diagnostics and effect estimates for each cohort comparison and outcome.
Open access to this app will be through a general public facing BETTER web-page.

## Scientific meetings and publications

We will deliver multiple presentations at scientific venues and will also prepare multiple scientific publications for clinical, informatics and statistical journals.

## General public

We believe in sharing our findings that will guide clinical care with the general public.
BETTER will use social-media (Twitter) to facilitate this.
With dedicated support from the OHDSI communications specialist, we will deliver regular press releases at key project stages, distributed via the extensive media networks of UCLA and Columbia.

# References {-}

<div id="refs"></div>

# (APPENDIX) Appendix {-}

# Exposure Cohort Definitions

```{r h1n1-cohort, echo=FALSE, results="asis", warning=FALSE, message=FALSE}
baseCohortJson <- SqlRender::readSql(system.file("cohorts", "H1N1vaccination.json", package = "better"))
baseCohort <- RJSONIO::fromJSON(baseCohortJson)

baseCohortJson <- RJSONIO::toJSON(baseCohort, digits = 50)

printCohortDefinitionFromNameAndJson(name = "H1N1pdm Vaccines",
                                     json = baseCohortJson)
```

```{r fluvirin-cohort, echo=FALSE, results="asis", warning=FALSE, message=FALSE}
baseCohortJson <- SqlRender::readSql(system.file("cohorts", "Fluvirin.json", package = "better"))
baseCohort <- RJSONIO::fromJSON(baseCohortJson)

baseCohortJson <- RJSONIO::toJSON(baseCohort, digits = 50)

printCohortDefinitionFromNameAndJson(name = "Seasonal Flu Vaccines (Fluvirin)",
                                     json = baseCohortJson)
```

```{r fluzone-cohort, echo=FALSE, results="asis", warning=FALSE, message=FALSE}
baseCohortJson <- SqlRender::readSql(system.file("cohorts", "Fluzone.json", package = "better"))
baseCohort <- RJSONIO::fromJSON(baseCohortJson)

baseCohortJson <- RJSONIO::toJSON(baseCohort, digits = 50)

printCohortDefinitionFromNameAndJson(name = "Seasonal Flu Vaccines (Fluzone)",
                                     json = baseCohortJson)
```

```{r flu-cohort, echo=FALSE, results="asis", warning=FALSE, message=FALSE}
baseCohortJson <- SqlRender::readSql(system.file("cohorts", "AllFluVaccines.json", package = "better"))
baseCohort <- RJSONIO::fromJSON(baseCohortJson)

baseCohortJson <- RJSONIO::toJSON(baseCohort, digits = 50)

printCohortDefinitionFromNameAndJson(name = "Seasonal Flu Vaccines (All)",
                                     json = baseCohortJson)
```

```{r hpv-cohort, echo=FALSE, results="asis", warning=FALSE, message=FALSE}
baseCohortJson <- SqlRender::readSql(system.file("cohorts", "Gardasil9.json", package = "better"))
baseCohort <- RJSONIO::fromJSON(baseCohortJson)

baseCohortJson <- RJSONIO::toJSON(baseCohort, digits = 50)

printCohortDefinitionFromNameAndJson(name = "HPV Vaccines",
                                     json = baseCohortJson)
```

```{r zoster-cohort, echo=FALSE, results="asis", warning=FALSE, message=FALSE}
baseCohortJson <- SqlRender::readSql(system.file("cohorts", "Shingrix.json", package = "better"))
baseCohort <- RJSONIO::fromJSON(baseCohortJson)

baseCohortJson <- RJSONIO::toJSON(baseCohort, digits = 50)

printCohortDefinitionFromNameAndJson(name = "Zoster Vaccines",
                                     json = baseCohortJson)
```


# Negative controls

```{r ncs, echo=FALSE, warning=FALSE}
ncs <- readr::read_csv(system.file("settings", "NegativeControls.csv", package = "better"), col_types = readr::cols())
colnames(ncs) <- SqlRender::camelCaseToTitleCase(colnames(ncs))

tab <- kable(ncs, booktabs = TRUE, linesep = "",
      caption = "Negative control outcomes.", longtable = TRUE) %>% 
  kable_styling(bootstrap_options = "striped", latex_options = "striped")

if (knitr::is_latex_output()) {
  tab %>%
    column_spec(1, width = "15em") %>%
    column_spec(2, width = "40em") %>%
    kable_styling(font_size = latex_table_font_size)
} else {
  tab
}
```

# Additional investigated outcome cohort

```{r guillainBarre-cohort, echo=FALSE, results="asis", warning=FALSE, message=FALSE}
baseCohortJson <- SqlRender::readSql(system.file("cohorts", "GBS.json", package = "better"))
baseCohort <- RJSONIO::fromJSON(baseCohortJson)
baseCohortJson <- RJSONIO::toJSON(baseCohort, digits = 50)
printCohortDefinitionFromNameAndJson(name = "Adverse Event Outcome - Guillain Barre Syndrome",
                                     json = baseCohortJson)
```
